***************SVM
1.svm对噪声的鲁棒性：
SVM本身对噪声具有一定的鲁棒性，但实验证明，是当噪声率低于一定水平的噪声对SVM没有太大影响，但随着噪声率的不断增加，分类器的识别率会降低。

2. svm高斯核函数比线性核函数模型更复杂，容易过拟合（高斯核是无穷维的）
实质上是度量了两个样本的相似度
gamma越大越容易过拟合
当gamma趋向于无穷：SVM+RBF = KNN


3.SVM核函数包括线性核函数、多项式核函数、高斯核函数、幂指数核函数、拉普拉斯核函数、ANOVA核函数、二次有理核函数、多元二次核函数、逆多元二次核函数以及Sigmoid核函数

4.线性SVM目标函数中的松弛因子的系数C越大越容易过拟合

**********************SVM与Logistic回归
**SVM与Logistic的相同点：
1、都是分类算法
2、如果不考虑核函数，LR和SVM都是线性分类模型，求解分类超平面
3、都是监督学习方法
4、都是判别模型
5、都在学术界和工业界广为人知并且应用广泛

**SVM和LR的不同点
1、loss function不一样（本质不同）
LR基于概率理论，损失函数是logistica loss，通过极大似然估计方法估计出参数的值，然后计算分类概率，取概率较大的作为分类结果；SVM基于几何间隔最大化，损失函数是hinge loss，把最大几何间隔面作为最优分类面（或者化为对偶问题即拉格朗日函数的极大极小值问题）
2、这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重，根本目的是一样的，但是处理方式不同。SVM只考虑离分类面最近的点，即支持向量，所以线性SVM不直接依赖于数据分布；LR通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重，这样则考虑了所有的点，与分类面距离较远的点对结果也起作用，虽然作用较小，如果数据不同类别strongly unbalance，一般需要先对数据做balancing。
这也使得当一个数据集已经被Linear SVM求解，那么往这个数据集里面增加或者删除更多的一类点并不会改变重新求解的Linear SVM平面。
3、在解决非线性问题时，SVM采用核函数，而LR通常不采用
分类模型的结果就是计算决策面，模型训练的过程就是决策面的计算过程。在计算决策面时，SVM算法只有支持向量参与了核计算，即kernel machine的解的系数是稀疏的。在LR算法中，如果采用核函数，则每一个样本点都会参与核计算，这会带来很高的计算复杂度，所以LR很少采用核函数。
4、SVM不具有伸缩不变性，LR则具有伸缩不变性（对数据和参数的敏感程度不同，SVM计算margin是依赖数据表达上的距离测度的，当这个测度不好（高维尤为明显），求得所谓的large margin就没有意义了）
SVM在各个维度进行不均匀伸缩后，最优解与原来不等价，除非本来各维数据的分布范围比较接近，否则必须进行标准化。LR模型在各个维度进行不均匀伸缩后，最优解与原来等价，是否标准化理论上不会改变最优解，但是，由于实际求解使用迭代算法，如果不做标准化，迭代算法可能收敛的很慢甚至不收敛。（不带正则化的LR，其做normalization的目的是为了方便选择优化过程的起始值）
5、SVM损失函数自带正则项，是结构风险最小化算法，LR需要额外在损失函数上加正则项

**LR的特点
logistic回归可以给出样本x的后验概率
方便给出概率，并且向 multi-class 的扩展更加直接
模型更简单，好理解，速度快，实现起来特别是大规模线性分类时比较方便
对缺失值敏感
LR则具有伸缩不变性，对数据和参数更稳定
由于都是线性分类器，实际上对低维数据的overfitting能力都比较有限，相比之下对高维数据，LR的表现为更加稳定。所以SVM一般需要做normalization，而LR不敏感
**SVM的特点
svm的拟合效果和泛化能力都更好
理解和优化相对复杂，但是SVM的理论基础更加牢固（结构风险最小化）
转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，在进行复杂核函数计算时优势明显，大大简化模型和计算量
只考虑离分类面最近的点，即支持向量，所以线性SVM不直接依赖于数据分布
增加或者删除更多的一类点并不会改变重新求解的Linear SVM平面


****适用环境
如果feature的数量很大，跟样本数量差不多，选用LR或者是Linear Kernel的SVM
如果feature的数量比较小，样本数量一般，不大也不小，选用Gaussian kernel的SVM
如果feature的数量比较小，而样本数量很多，需要手工添加feature变成第一种情况
对于海量数据，LR的使用更加广泛
如果数据集本身比较小而且维度高的话一般 SVM 表现更好
LR模型受数据分布影响，如果数据不同类别strongly unbalance，一般需要先对数据做balancing。
如果数据维度很高，LR模型都会配合参数的L1 regularization
使用Linear SVM之前一般都需要先对数据做normalization，而LR不敏感
LR引入非线性：1、特征分箱+onehot（可以引入非线性信息；onehot可以提高LR的计算能力；同时还能处理缺失值特征；还能提高对异常值的鲁棒性）2、特征项引入n次方
LR处理共线性特征:1、pearson相关系数，特征选择 2、GBDT+LR，用树模型构造特征


***********boosting与bagging
1.bagging与boosting的区别：
a.采样方式：Adaboost是错误分类的样本的权重较大实际是每个样本都会使用；                   Bagging采用有放回的随机采样；
b.基分类器的权重系数：Adaboost中错误率较低的分类器权重较大；Bagging中                   采用投票法，所以每个基分类器的权重系数都是一样的。
c.Bagging的各个预测函数可以并行生成，而Boosing的各个预测函数只能顺序                   生成。
d.Bias-variance权衡：boosting可以减少bias，每一次boost都能使模型更加接                        近中心，即总分类器的拟合能力更好； Bagging可以减少训练方                       差variance（var/m），即总分类器对数据扰动的承受能力更强                      ，并能减少过拟合，对不剪枝的决策树、神经网络等有良好的集成                       效果


GBDT减少过拟合的方法
1.正则化（复杂度可以正比于叶节点数或者叶节点预测值的平方和，可以用于剪枝）
2.叶节点数目控制数的层数
3.叶节点包含的最少样本数（可以降低预测方差）
4.控制迭代次数
5.交叉验证

衰减
Fm(x) = Fm-1(x) + v*gama*fm(x)   v为学习率，gama为权重

随机梯度提升
每次迭代都对伪残差样本采用无放回的降采样
也能够防止过拟合，并且收敛快，额外好处是能够用剩余的样本做验证




*************************XGBoost和Lightgbm
lgb可以直接使用分类特征，而不用进行one-hot编码

得到更好训练精度的方法：1.小的学习率和大的迭代次数 2.大的叶子结点数 3.cross validation 4.更大的数据 5.drop out

lgb处理过拟合的方法
1.small max_bin  分桶略粗一些
2.small num_leaves 不要在单棵树上分的太细
**3.确保叶子节点还有足够多的数据
4.sub_sample   在data上做一些sample
**5.sub_feature   在feature上做一些sample
6.更多的训练数据
7.正则 lambda_l1,lambda_l2,min_gain_to_split
8.控制树深度

xgb的不足：
每轮迭代时，都需要遍历整个训练数据多次
预排序方法（pre-sorted）空间消耗大，需要保存数据的特征值，还保存了特征排序的结果，其次时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。且容易导致过拟合。
对cache优化不友好，对梯度和行索引到叶子索引数组的随机访问，并且不同的特征访问的顺序不一样，会造成较大的cache miss

lgb的优势：
更快的训练效率
低内存使用
更高的准确率
支持并行化学习
可处理大规模数据

lgb的特点：
基于Histogram的决策树算法
带深度限制的Leaf-wise的叶子生长策略
直方图做差加速
直接支持类别特征(Categorical Feature)
Cache命中率优化（顺序访问梯度）
基于直方图的稀疏特征优化
直接支持高效并行（特征并行，数据并行，基于投票的数据并行）
lightgbm保留高梯度的samples，而从导致较小的梯度变化的samples中进行一个随机采样


Histogram算法：直方图算法的基本思想：先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。遍历数据时，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。

Histogram算法的优点：
内存消耗的降低，直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值。
计算上的代价也大幅降低，XGBoost预排序算法每遍历一个特征值就需要计算一次分裂的增益，而直方图算法只需要计算k次
能起到正则化的效果，可以预防过拟合，有利于推广

Histogram算法的缺点：
对结果的精度会产生一些影响。但其实影响并不是很大，原因是决策树本来就是弱模型，分割点是不是精确并不是太重要，即使单棵树的训练误差比精确分割的算法稍大，但在梯度提升的框架下没有太大的影响。

Level-wise遍历一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效算法，进行不必要的的搜索和分裂
Leaf-wise则是一种更为高效的策略：找到分裂增益最大的一个叶子，然后分裂，因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。


Leaf-wise的缺点：可能会长出比较深的决策树，产生过拟合。因此需要增加一个最大深度限制，在保证高效率的同时防止过拟合。







****************KNN
1.Knn算法，适合于样本数较少，典型性较好的样本集
样本数少：kNN每次预测要计算距离，所以是带着整个样本集跑的（也有些剪辑近邻之类的会剪掉一些），所以样本数越少越好。

典型性：不仅是kNN，每个分类算法都希望样本典型性好，这样才好做分类。

2.优点：精度高，对异常值不敏感，无数据输入假定
   缺点：计算复杂度高，空间复杂度高
   适用数据：数值型和标称型



********************决策树
1.优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据
    缺点：可能会出现过度匹配问题
    适用数据：数值型和标称型

https://blog.csdn.net/choven_meng/article/details/82878018

ID3算法的缺点：
（1）不能对连续数据进行处理，只能通过连续数据离散化进行处理；
（2）采用信息增益进行数据分裂容易偏向取值较多的特征，准确性不如信息增益率；
（3）缺失值不好处理。
（4）没有采用剪枝，决策树的结构可能过于复杂，出现过拟合。

C4.5解决了上述的问题：
（1）从小到大排列为a1,a2,a3......am,则C4.5取相邻两样本值的平均值，一共取得m-1个划分点
（2）C4.5采用信息增益率的方法，它是信息增益和特征熵的比值，特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益偏向取值较多的特征的问题。
（3）含缺失值特征的信息增益 = 无缺失值样本所占的比例 * 无缺失值样本的信息增益
（4）在树的构造过程中会进行剪枝操作优化
优点：产生的规则易于理解；准确率较高；实现简单；
缺点： 对数据进行多次顺序扫描和排序，效率较低；只适合小规模数据集，需要将数据放到内存中。

CART
对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则 ，进行特征选择，生成二叉树。









******************CRF模型（条件随机场）、HMM模型与MEMM模型（最大熵马尔科夫）
1.三者比较的优劣：
1)   CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息          。特征设计灵活（与ME一样） ————与HMM比较
2）同时，由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔可      夫模型标记偏置（Label-bias）的缺点。 ­­————与MEMM比较
3）CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分      布，而不是在给定当前状态条件下，定义下一个状态的状态分布。
       ——与ME比较
缺点：训练代价大、复杂度高


***************关联规则
1.置信度计算规则为： 同时购买商品A和商品B的交易次数/购买了商品A的次数
   支持度计算规则为： 同时购买了商品A和商品B的交易次数/总的交易次数



************模式识别分类（贝叶斯决策）
在贝叶斯决策中，对于先验概率p(y)，分为已知和未知两种情况。
1. p(y)已知，直接使用贝叶斯公式求后验概率即可；
2. p(y)未知，可以使用聂曼-皮尔逊决策(N-P决策)来计算决策面。
而最大最小损失规则主要就是用来解决使用最小损失规则时先验概率未知或难以计算的问题的

***************贝叶斯网络
1.互信息 I（X,Y）= D(P(X,Y) || P(X)P(Y))   D为相对熵（D（p||q）=sigma（p*log（p/q）））
当随机变量X，Y相互独立时，互信息为0
互信息相当于ID3中的信息增益

2.贝叶斯公式可以转化为极大似然估计
maxP(Ai|D) ---> maxP(D|Ai)   D为样本，Ai为参数取值，这里假设参数的各个取值的概率近似相等

3.贝叶斯网络 又叫做有向无环图模型 是一种概率图模型

4.概率无向图模型是马尔科夫网络（马尔科夫随机场）

5.判定条件独立：（单节点）
当C已知：tail-to-tail   head-to-tail
当C未知：head-to-tail

节点集：有向分割（D-separation）

6.贝叶斯网络中某节点的markov blanket是其所有的父节点、子节点、以及子节点的其他父节点

7.朴素贝叶斯进行文本分类时无法解决语料中一词多义和多词一义的问题
可以通过增加“主题”的方式解决

******************主题模型
1.主题模型是一个降维的过程，也可以看做soft聚类

2.gamma函数是阶乘在实数域上的推广
gamma(x) = (x-1)!
gamma(x) = (x-1)gamma(x-1)

3.Beta分布的期望：α/(α+β)
Dirichlet分布是Beta分布的推广（2维到多维）期望：αi/(sigma(αi))
当α=β=1，Beta分布是（0,1）的均匀分布（1维均匀分布）


4.如果参数的先验分布与后验分布（似然*先验）属于同一个分布，称为共轭先验分布 （如：二项分布的共轭先验分布是Beta分布）
先验分布为Beta（α，β），后验分布为Beta(X+α，N-X+β)（X表示成功的次数），参数的后验估计为 E=(α+X)/(α+β+N)，当α，β都为0时，估计正好是极大似然估计；不为0时，称作伪计数。

5.推广
二项分布的共轭先验分布是Beta分布
多项分布的共轭先验分布是Dirichlet分布
先验分布为Dirichlet（K，α），后验分布为Dirichlet（K，ci+α）（ci为样本数）

6.对称Dirichlet分布（αi相等）的参数
α=1时：退化为均匀分布
α>1时：pi相等的概率增大
α<1时：pi=1，p非i=0的概率增大
（也就是说调小α可以增强主题的鲜明度，但是当样本足够大，影响会减小）


**LDA（隐Dirichlet分布，是一个贝叶斯网络）
每篇文章有各自的主题分布（K多项分布），主题分布的参数服从Dirichlet分布，Dirichlet分布参数是α
每个主题有各自的词分布（V多项分布，V是词典），词分布的参数也服从Dirichlet分布，Dirichlet分布参数是β


*************过拟合的原因
1、训练数据不足，有限的训练数据
2、训练模型过度导致模型非常复杂，泛化能力差
（过拟合表现为方差大）



*************判别模型与生成模型
对于输入x，类别标签y：
产生式模型估计它们的联合概率分布P(x,y)
判别式模型估计条件概率分布P(y|x)

产生式模型可以根据贝叶斯公式得到判别式模型，但反过来不行。





*************特征选择
1.常见的六种特征选择方法：
1）DF(Document Frequency) 文档频率
DF:统计特征词出现的文档数量，用来衡量某个特征词的重要性
2）MI(Mutual Information) 互信息法
互信息法用于衡量特征词与文档类别直接的信息量。
如果某个特征词的频率很低，那么互信息得分就会很大，因此互信息法倾向"低频"的特征词。
相对的词频很高的词，得分就会变低，如果这词携带了很高的信息量，互信息法就会变得低效。
3）(Information Gain) 信息增益法
通过某个特征词的缺失与存在的两种情况下，语料中前后信息的增加，衡量某个特征词的重要性。
4）CHI(Chi-square) 卡方检验法
利用了统计学中的"假设检验"的基本思想：首先假设特征词与类别直接是不相关的
如果利用CHI分布计算出的检验值偏离阈值越大，那么更有信心否定原假设，接受原假设的备则假设：特征词与类别有着很高的关联度。
5）WLLR(Weighted Log Likelihood Ration)加权对数似然
6）WFO（Weighted Frequency and Odds）加权频率和可能性
7)  期望交叉熵

主成分分析是特征转换算法（特征抽取），而不是特征选择




*********************线性分类器的三种最优准则
感知准则函数 ：准则函数以使错分类样本到分界面距离之和最小为原则。
支持向量机 ：基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的间隔为最大
Fisher准则：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大




********************核函数
所谓判别式模型，需要把正负样本区分开，那势必会遇到区分不开的情形，这时就要用到核函数了，所以可以认为判别式模型都要用核函数的。




******************损失函数
决定用什么损失函数需要看y服从什么样的分布，当y服从高斯分布，一般选择平方和损失函数，当y服从拉布拉斯分布，一般选择绝对值损失函数
正则化同理

平均数是平方和最小的最优解
中位数是绝对值和最小的最优解




***************AUC
最直观的有两种计算AUC的方法


1：绘制ROC曲线，ROC曲线下面的面积就是AUC的值


2：假设总共有（m+n）个样本，其中正样本m个，负样本n个，总共有m*n个样本对，计数，正样本预测为正样本的概率值大于负样本预测为正样本的概率值记为1，累加计数，然后除以（m*n）就是AUC的值





***************聚类
1.聚类和降维都是无监督的对于x的自相似性做的变换



***************K-means
能够非常方便的将为标记的样本分成若干簇，但是无法给出某个样本属于该簇的后验概率。

对异常值敏感，当有异常值时，可以使用k-Mediods（k中值聚类）
对初值敏感（可以使用2分k-means）可以根据与之前中心的距离加权选择新的中心（k-means++）

k-means中心点的更新本质上是对平方损失函数的梯度下降（梯度下降只能找到局部最优解，所以解释了初值敏感）,默认了样本服从高斯混合分布
高斯分布只能得到类似圆形的簇，且方差不相等效果不一定好
数量不相等的k-means聚类也不一定合适
当选择部分点求均值更新中心点时，就是随机梯度下降，叫做mini-batch k-means

k-mediods中心点更新是对绝对值损失函数的梯度下降，默认样本服从拉布拉斯混合分布

**优点
是解决聚类问题的一种经典算法，简单，快速
对处理大数据集，保持可伸缩性和高效率
当簇近似为高斯分布时，效果好
**缺点
在簇的平均值可被定义的情况下才能使用
必须事先给出k，而且对初值敏感
不适合于发现非凸形状的簇或者大小差别很大的簇
对噪声和孤立点数据敏感


***************密度聚类
**DBSCAN
只要样本点的密度大于某阈值，则将该样本添加到最近的簇中（密度相连）

基于密度的聚类可以得到边界（无法与新的点密度相连的点，也叫做光晕），边界的点的聚类可靠性低，但无法得到具体地后验概率，而基于EM算法的概率模型可以得到（如GMM）
可以克服基于距离的算法只能发现类圆形的聚类的缺点，可以发现任意形状
且对噪声数据不敏感
但计算密度单元复杂度大，需要建立空间索引

**密度最大值聚类
根据 ρ（局部密度）和delta（高局部密度点距离）的值确定簇的中心，再用DBSCAN或者k-means进行聚类
可以识别噪声

************************谱聚类
谱：方阵作为线性算子，它的所有特征值的全体称为方阵的谱
谱半径：最大的特征值
矩阵A的谱：A转置*A的特征值

谱聚类是一种基于图论的聚类方法，通过对样本数据的拉普拉斯矩阵的特征向量进行聚类，从而达到对样本数据聚类的目的。

谱聚类可以看做是一种降维

步骤：
计算相似度Sij，得到相似度矩阵W（邻接矩阵，设置对角线为0，且对称，首先选用k近邻图）
再计算度矩阵D（对角阵，每个元素为顶点到其他点的相似度之和）
a、未正则拉普拉斯矩阵：L=D-W（对称阵，且半正定）
求L的特征值、特征向量（最小特征值是0，相应的特征向量是全1向量）
将特征值从小到大排序，若分成k个簇，则取前k个特征的特征向量（列向量）组成U矩阵（n*k），每一行作为一个样本的特征（用特征值，特征向量的方法选取样本的特征），对n个特征进行k-means聚类（为了进行离散化），作为最终的聚类
k必须事先给定（可以考虑前k个较小的特征值的数量）
b、正则的拉普拉斯矩阵
*1.随机游走拉普拉斯矩阵：Lrw=D逆*（D-W），其他做法相同（推荐）
2.对称拉普拉斯矩阵：Lsym=D^(-1/2)*(D-W)*D^(-1/2),这时需要将每一行的特征进行单位化，再聚类







****************neural network
1.当使用平方和损失函数时，权重w和偏置b的梯度和激活函数的梯度成正比，激活函数的梯度越大，参数的更新速度越快，训练收敛的越快。当激活函数的梯度为0，会导致梯度消失的问题。

2.当使用交叉熵损失函数时，权值与偏置的调整与激活函数的梯度无关，且当输出值与实际值的误差越大，参数的更新速度越快。
所以如果输出神经元是线性的，适合用平方损失函数；如果输出神经元是S型函数的，适合用交叉熵损失函数
3.当输出层激活函数是softmax时，适合用对数似然代价函数（是交叉熵的推广，当softmax为二分类时，对数似然代价函数就是交叉熵损失函数）


https://blog.csdn.net/xwd18280820053/article/details/76026523
















































































